{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal Clause Semantic Similarity Detection - GPU Optimized\n",
        "## Baseline NLP Architectures with GPU Acceleration\n",
        "\n",
        "**Author:** NLP Expert Implementation  \n",
        "**Date:** November 2025  \n",
        "**Dataset:** Legal Clause Dataset from Kaggle  \n",
        "**Hardware:** GPU-Accelerated Training with Mixed Precision\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ GPU Optimizations Included\n",
        "\n",
        "1. **Automatic GPU Detection and Configuration**\n",
        "2. **Memory Growth Management** - Prevents OOM errors\n",
        "3. **Mixed Precision Training** - 2-3x faster training\n",
        "4. **XLA Compilation** - Just-In-Time optimization\n",
        "5. **Data Pipeline Optimization** - `tf.data` with prefetching\n",
        "6. **Batch Size Optimization** - Larger batches for GPU\n",
        "7. **Multi-GPU Support** - Automatic distribution strategy\n",
        "\n",
        "### üìã Assignment Overview\n",
        "\n",
        "This notebook implements two baseline NLP models to detect semantic similarity between legal clauses:\n",
        "1. **BiLSTM Siamese Network** - Shared encoder architecture\n",
        "2. **Attention-Based Encoder** - Self-attention mechanism\n",
        "\n",
        "### üö´ Constraints\n",
        "- No pre-trained transformers (BERT, RoBERTa, Legal-BERT)\n",
        "- Only TensorFlow/Keras built-in layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. GPU Configuration and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"GPU Configuration and Environment Setup.\"\"\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# ============================================================================\n",
        "# GPU CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "def configure_gpu():\n",
        "    \"\"\"\n",
        "    Configure TensorFlow for optimal GPU usage.\n",
        "    \n",
        "    Optimizations:\n",
        "    1. Enable memory growth to prevent TensorFlow from allocating all GPU memory\n",
        "    2. Set up mixed precision for faster training (2-3x speedup)\n",
        "    3. Enable XLA compilation for optimized operations\n",
        "    4. Configure multi-GPU strategy if available\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"GPU CONFIGURATION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # List available GPUs\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    print(f\"\\nüîç Detecting GPUs...\")\n",
        "    print(f\"Number of GPUs Available: {len(gpus)}\")\n",
        "    \n",
        "    if gpus:\n",
        "        try:\n",
        "            # Enable memory growth for all GPUs\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                print(f\"‚úì Memory growth enabled for: {gpu.name}\")\n",
        "            \n",
        "            # Get GPU details\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                print(f\"\\nGPU {i}: {gpu.name}\")\n",
        "                print(f\"  Type: {gpu.device_type}\")\n",
        "            \n",
        "            # Enable Mixed Precision Training\n",
        "            print(\"\\nüöÄ Enabling Mixed Precision Training (float16)...\")\n",
        "            policy = keras.mixed_precision.Policy('mixed_float16')\n",
        "            keras.mixed_precision.set_global_policy(policy)\n",
        "            print(f\"‚úì Compute dtype: {policy.compute_dtype}\")\n",
        "            print(f\"‚úì Variable dtype: {policy.variable_dtype}\")\n",
        "            print(\"‚úì Mixed precision enabled - Expected 2-3x speedup!\")\n",
        "            \n",
        "            # Enable XLA (Accelerated Linear Algebra)\n",
        "            print(\"\\n‚ö° Enabling XLA Compilation...\")\n",
        "            tf.config.optimizer.set_jit(True)\n",
        "            print(\"‚úì XLA JIT compilation enabled\")\n",
        "            \n",
        "            # Set up distribution strategy for multi-GPU\n",
        "            if len(gpus) > 1:\n",
        "                print(f\"\\nüîÑ Setting up Multi-GPU Strategy ({len(gpus)} GPUs)...\")\n",
        "                strategy = tf.distribute.MirroredStrategy()\n",
        "                print(f\"‚úì MirroredStrategy initialized\")\n",
        "                print(f\"‚úì Number of devices: {strategy.num_replicas_in_sync}\")\n",
        "                return strategy\n",
        "            else:\n",
        "                print(\"\\n‚úì Single GPU mode\")\n",
        "                return None\n",
        "                \n",
        "        except RuntimeError as e:\n",
        "            print(f\"\\n‚ö†Ô∏è GPU configuration error: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No GPU detected. Running on CPU.\")\n",
        "        print(\"   For GPU support, ensure:\")\n",
        "        print(\"   1. NVIDIA GPU with CUDA support\")\n",
        "        print(\"   2. CUDA Toolkit installed (11.2+)\")\n",
        "        print(\"   3. cuDNN library installed (8.1+)\")\n",
        "        print(\"   4. TensorFlow-GPU installed: pip install tensorflow[and-cuda]\")\n",
        "        return None\n",
        "\n",
        "# Configure GPU\n",
        "strategy = configure_gpu()\n",
        "\n",
        "# Display TensorFlow build information\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TENSORFLOW BUILD INFO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "print(f\"CUDA available: {tf.test.is_built_with_cuda()}\")\n",
        "print(f\"GPU available: {tf.test.is_gpu_available()}\" if hasattr(tf.test, 'is_gpu_available') else \"GPU check: Use tf.config.list_physical_devices('GPU')\")\n",
        "print(\"\\n‚úÖ GPU Configuration Complete!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import re\n",
        "import glob\n",
        "import warnings\n",
        "import time\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# TensorFlow/Keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, LSTM, Bidirectional, Dense, Dropout, \n",
        "    Lambda, Concatenate, Multiply, Attention,\n",
        "    GlobalAveragePooling1D, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(\"\\n‚úÖ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. GPU-Optimized Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPUConfig:\n",
        "    \"\"\"GPU-optimized configuration for hyperparameters and settings.\"\"\"\n",
        "    \n",
        "    # Data paths\n",
        "    DATA_DIR = '.'\n",
        "    \n",
        "    # Text preprocessing\n",
        "    MAX_VOCAB_SIZE = 20000\n",
        "    MAX_SEQUENCE_LENGTH = 200\n",
        "    OOV_TOKEN = '<OOV>'\n",
        "    \n",
        "    # Model architecture\n",
        "    EMBEDDING_DIM = 128\n",
        "    LSTM_UNITS = 128\n",
        "    ATTENTION_UNITS = 64\n",
        "    DENSE_UNITS = 64\n",
        "    DROPOUT_RATE = 0.3\n",
        "    \n",
        "    # GPU-Optimized Training Parameters\n",
        "    BATCH_SIZE = 128  # Increased for GPU (was 64)\n",
        "    EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    VALIDATION_SPLIT = 0.2\n",
        "    TEST_SPLIT = 0.2\n",
        "    \n",
        "    # Data pipeline optimization\n",
        "    PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE  # Auto-tune prefetching\n",
        "    NUM_PARALLEL_CALLS = tf.data.AUTOTUNE  # Auto-tune parallel processing\n",
        "    \n",
        "    # Pair generation\n",
        "    POSITIVE_PAIRS_PER_CATEGORY = 100\n",
        "    NEGATIVE_SAMPLE_RATIO = 1.0\n",
        "    \n",
        "    # Callbacks\n",
        "    EARLY_STOPPING_PATIENCE = 10\n",
        "    REDUCE_LR_PATIENCE = 5\n",
        "    \n",
        "    # Mixed precision\n",
        "    USE_MIXED_PRECISION = True\n",
        "    \n",
        "    # XLA compilation\n",
        "    USE_XLA = True\n",
        "\n",
        "config = GPUConfig()\n",
        "\n",
        "print(\"GPU-Optimized Configuration:\")\n",
        "print(f\"  Max Vocabulary Size: {config.MAX_VOCAB_SIZE}\")\n",
        "print(f\"  Max Sequence Length: {config.MAX_SEQUENCE_LENGTH}\")\n",
        "print(f\"  Batch Size (GPU-optimized): {config.BATCH_SIZE}\")\n",
        "print(f\"  Embedding Dimension: {config.EMBEDDING_DIM}\")\n",
        "print(f\"  LSTM Units: {config.LSTM_UNITS}\")\n",
        "print(f\"  Mixed Precision: {config.USE_MIXED_PRECISION}\")\n",
        "print(f\"  XLA Compilation: {config.USE_XLA}\")\n",
        "print(f\"  Data Prefetching: AUTOTUNE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_legal_clauses(data_dir: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load all CSV files from the data directory and combine into a single DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing CSV files\n",
        "        \n",
        "    Returns:\n",
        "        Combined DataFrame with all legal clauses\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
        "    \n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in {data_dir}\")\n",
        "    \n",
        "    print(f\"Found {len(csv_files)} CSV files\")\n",
        "    \n",
        "    all_data = []\n",
        "    \n",
        "    for file_path in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "            all_data.append(df)\n",
        "            print(f\"‚úì Loaded: {os.path.basename(file_path)} - {len(df)} clauses\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Error loading {file_path}: {e}\")\n",
        "    \n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "# Load data\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING LEGAL CLAUSE DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_clauses = load_legal_clauses(config.DATA_DIR)\n",
        "\n",
        "print(f\"\\nTotal clauses loaded: {len(df_clauses)}\")\n",
        "print(f\"Columns: {list(df_clauses.columns)}\")\n",
        "print(f\"DataFrame Shape: {df_clauses.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data exploration\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA EXPLORATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check for missing values and clean\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df_clauses.isnull().sum())\n",
        "\n",
        "df_clauses = df_clauses.dropna()\n",
        "print(f\"\\nClauses after removing NaN: {len(df_clauses)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample Clauses:\")\n",
        "print(df_clauses.head())\n",
        "\n",
        "# Clause type distribution\n",
        "print(\"\\nClause Type Distribution:\")\n",
        "clause_type_counts = df_clauses['clause_type'].value_counts()\n",
        "print(clause_type_counts.head(20))\n",
        "print(f\"\\nTotal unique clause types: {df_clauses['clause_type'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "top_20_types = df_clauses['clause_type'].value_counts().head(20)\n",
        "plt.subplot(1, 2, 1)\n",
        "top_20_types.plot(kind='barh', color='steelblue')\n",
        "plt.xlabel('Number of Clauses')\n",
        "plt.ylabel('Clause Type')\n",
        "plt.title('Top 20 Most Common Clause Types')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "df_clauses['text_length'] = df_clauses['clause_text'].str.len()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df_clauses['text_length'], bins=50, color='coral', edgecolor='black')\n",
        "plt.xlabel('Character Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Clause Text Length')\n",
        "plt.axvline(df_clauses['text_length'].mean(), color='red', linestyle='--', label='Mean')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nText Length Statistics:\")\n",
        "print(df_clauses['text_length'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess legal text by cleaning and normalizing.\n",
        "    \n",
        "    Args:\n",
        "        text: Raw text string\n",
        "        \n",
        "    Returns:\n",
        "        Cleaned text string\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = text.lower()\n",
        "    text = ' '.join(text.split())\n",
        "    text = re.sub(r'[^a-z0-9\\s\\.,;:\\-]', '', text)\n",
        "    text = re.sub(r'([.,;:]){2,}', r'\\1', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "print(\"Preprocessing clause texts...\")\n",
        "df_clauses['clause_text_clean'] = df_clauses['clause_text'].apply(preprocess_text)\n",
        "df_clauses = df_clauses[df_clauses['clause_text_clean'].str.len() > 0]\n",
        "\n",
        "print(f\"Clauses after preprocessing: {len(df_clauses)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Original: {df_clauses['clause_text'].iloc[0][:200]}\")\n",
        "print(f\"Cleaned:  {df_clauses['clause_text_clean'].iloc[0][:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Training Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_clause_pairs(df: pd.DataFrame, \n",
        "                         max_positive_per_category: int = 100,\n",
        "                         negative_ratio: float = 1.0) -> Tuple[List, List, List]:\n",
        "    \"\"\"\n",
        "    Generate positive (similar) and negative (different) clause pairs.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with clause_text_clean and clause_type columns\n",
        "        max_positive_per_category: Maximum positive pairs per category\n",
        "        negative_ratio: Ratio of negative to positive pairs\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (clause1_list, clause2_list, labels_list)\n",
        "    \"\"\"\n",
        "    clause1_list = []\n",
        "    clause2_list = []\n",
        "    labels_list = []\n",
        "    \n",
        "    grouped = df.groupby('clause_type')\n",
        "    \n",
        "    print(\"Generating positive pairs (same category)...\")\n",
        "    positive_count = 0\n",
        "    \n",
        "    for clause_type, group in grouped:\n",
        "        texts = group['clause_text_clean'].values\n",
        "        \n",
        "        if len(texts) < 2:\n",
        "            continue\n",
        "        \n",
        "        pairs_generated = 0\n",
        "        for i in range(len(texts)):\n",
        "            if pairs_generated >= max_positive_per_category:\n",
        "                break\n",
        "            for j in range(i + 1, len(texts)):\n",
        "                if pairs_generated >= max_positive_per_category:\n",
        "                    break\n",
        "                clause1_list.append(texts[i])\n",
        "                clause2_list.append(texts[j])\n",
        "                labels_list.append(1)\n",
        "                pairs_generated += 1\n",
        "                positive_count += 1\n",
        "    \n",
        "    print(f\"Generated {positive_count} positive pairs\")\n",
        "    \n",
        "    print(\"Generating negative pairs (different categories)...\")\n",
        "    negative_target = int(positive_count * negative_ratio)\n",
        "    negative_count = 0\n",
        "    \n",
        "    clause_types = list(grouped.groups.keys())\n",
        "    \n",
        "    while negative_count < negative_target:\n",
        "        type1, type2 = np.random.choice(clause_types, size=2, replace=False)\n",
        "        text1 = np.random.choice(grouped.get_group(type1)['clause_text_clean'].values)\n",
        "        text2 = np.random.choice(grouped.get_group(type2)['clause_text_clean'].values)\n",
        "        \n",
        "        clause1_list.append(text1)\n",
        "        clause2_list.append(text2)\n",
        "        labels_list.append(0)\n",
        "        negative_count += 1\n",
        "    \n",
        "    print(f\"Generated {negative_count} negative pairs\")\n",
        "    print(f\"Total pairs: {len(labels_list)}\")\n",
        "    print(f\"Class balance: {np.mean(labels_list):.2%} positive\")\n",
        "    \n",
        "    return clause1_list, clause2_list, labels_list\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING CLAUSE PAIRS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "clause1, clause2, labels = generate_clause_pairs(\n",
        "    df_clauses,\n",
        "    max_positive_per_category=config.POSITIVE_PAIRS_PER_CATEGORY,\n",
        "    negative_ratio=config.NEGATIVE_SAMPLE_RATIO\n",
        ")\n",
        "\n",
        "clause1 = np.array(clause1)\n",
        "clause2 = np.array(clause2)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"\\nDataset size: {len(labels)} pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Tokenization and Sequence Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_sequences(clause1: np.ndarray, \n",
        "                     clause2: np.ndarray, \n",
        "                     labels: np.ndarray,\n",
        "                     max_vocab_size: int,\n",
        "                     max_seq_length: int,\n",
        "                     oov_token: str) -> Tuple:\n",
        "    \"\"\"Tokenize and prepare sequences.\"\"\"\n",
        "    print(\"Tokenizing texts...\")\n",
        "    \n",
        "    all_texts = np.concatenate([clause1, clause2])\n",
        "    \n",
        "    tokenizer = Tokenizer(\n",
        "        num_words=max_vocab_size,\n",
        "        oov_token=oov_token,\n",
        "        lower=True\n",
        "    )\n",
        "    tokenizer.fit_on_texts(all_texts)\n",
        "    \n",
        "    clause1_seq = tokenizer.texts_to_sequences(clause1)\n",
        "    clause2_seq = tokenizer.texts_to_sequences(clause2)\n",
        "    \n",
        "    clause1_padded = pad_sequences(clause1_seq, maxlen=max_seq_length, padding='post', truncating='post')\n",
        "    clause2_padded = pad_sequences(clause2_seq, maxlen=max_seq_length, padding='post', truncating='post')\n",
        "    \n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "    \n",
        "    return clause1_padded, clause2_padded, labels, tokenizer\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TOKENIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "clause1_seq, clause2_seq, labels, tokenizer = prepare_sequences(\n",
        "    clause1, clause2, labels,\n",
        "    config.MAX_VOCAB_SIZE,\n",
        "    config.MAX_SEQUENCE_LENGTH,\n",
        "    config.OOV_TOKEN\n",
        ")\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"\\nFinal vocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. GPU-Optimized Data Pipeline with tf.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Create GPU-optimized data pipelines using tf.data API.\"\"\"\n",
        "\n",
        "# Split data\n",
        "indices = np.arange(len(labels))\n",
        "train_idx, test_idx = train_test_split(\n",
        "    indices, test_size=config.TEST_SPLIT, stratify=labels, random_state=SEED\n",
        ")\n",
        "\n",
        "X1_train = clause1_seq[train_idx]\n",
        "X2_train = clause2_seq[train_idx]\n",
        "y_train = labels[train_idx]\n",
        "\n",
        "X1_test = clause1_seq[test_idx]\n",
        "X2_test = clause2_seq[test_idx]\n",
        "y_test = labels[test_idx]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"GPU-OPTIMIZED DATA PIPELINE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTraining set: {len(y_train)}\")\n",
        "print(f\"Test set: {len(y_test)}\")\n",
        "\n",
        "def create_gpu_dataset(X1, X2, y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Create GPU-optimized tf.data dataset with prefetching.\n",
        "    \n",
        "    Optimizations:\n",
        "    - Prefetch: Overlaps data preprocessing and model execution\n",
        "    - Cache: Caches dataset in memory after first epoch\n",
        "    - Shuffle: Better generalization\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(((X1, X2), y))\n",
        "    \n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=10000)\n",
        "    \n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.cache()  # Cache in memory\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch for GPU\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Create training dataset\n",
        "train_dataset = create_gpu_dataset(X1_train, X2_train, y_train, \n",
        "                                  config.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Create validation split\n",
        "val_size = int(len(X1_train) * config.VALIDATION_SPLIT)\n",
        "X1_val = X1_train[-val_size:]\n",
        "X2_val = X2_train[-val_size:]\n",
        "y_val = y_train[-val_size:]\n",
        "\n",
        "X1_train_only = X1_train[:-val_size]\n",
        "X2_train_only = X2_train[:-val_size]\n",
        "y_train_only = y_train[:-val_size]\n",
        "\n",
        "train_dataset = create_gpu_dataset(X1_train_only, X2_train_only, y_train_only,\n",
        "                                  config.BATCH_SIZE, shuffle=True)\n",
        "val_dataset = create_gpu_dataset(X1_val, X2_val, y_val,\n",
        "                                config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"\\n‚úÖ GPU-optimized data pipeline created!\")\n",
        "print(f\"  ‚Ä¢ Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  ‚Ä¢ Prefetching: AUTOTUNE\")\n",
        "print(f\"  ‚Ä¢ Caching: Enabled\")\n",
        "print(f\"  ‚Ä¢ Training batches: {len(list(train_dataset))}\")\n",
        "print(f\"  ‚Ä¢ Validation batches: {len(list(val_dataset))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model 1: BiLSTM Siamese Network (GPU-Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_bilstm_siamese_model(vocab_size: int,\n",
        "                               embedding_dim: int,\n",
        "                               max_seq_length: int,\n",
        "                               lstm_units: int,\n",
        "                               dropout_rate: float = 0.3) -> Model:\n",
        "    \"\"\"\n",
        "    Build GPU-optimized BiLSTM Siamese Network.\n",
        "    \n",
        "    GPU Optimizations:\n",
        "    - CuDNN-optimized LSTM layers (automatic when GPU available)\n",
        "    - Mixed precision compatible architecture\n",
        "    - Batch normalization for faster convergence\n",
        "    \"\"\"\n",
        "    input_1 = Input(shape=(max_seq_length,), name='clause_1')\n",
        "    input_2 = Input(shape=(max_seq_length,), name='clause_2')\n",
        "    \n",
        "    # Shared embedding\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=max_seq_length,\n",
        "        mask_zero=True,\n",
        "        name='shared_embedding'\n",
        "    )\n",
        "    \n",
        "    # Shared BiLSTM (CuDNN-optimized on GPU)\n",
        "    bilstm_layer = Bidirectional(\n",
        "        LSTM(lstm_units, return_sequences=False, dropout=dropout_rate),\n",
        "        name='shared_bilstm'\n",
        "    )\n",
        "    \n",
        "    embedded_1 = embedding_layer(input_1)\n",
        "    embedded_2 = embedding_layer(input_2)\n",
        "    \n",
        "    encoded_1 = bilstm_layer(embedded_1)\n",
        "    encoded_2 = bilstm_layer(embedded_2)\n",
        "    \n",
        "    # Similarity features\n",
        "    difference = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([encoded_1, encoded_2])\n",
        "    multiplication = Multiply()([encoded_1, encoded_2])\n",
        "    merged = Concatenate()([difference, multiplication])\n",
        "    \n",
        "    # Dense layers with BatchNorm for GPU\n",
        "    dense1 = Dense(64, activation='relu')(merged)\n",
        "    dense1 = BatchNormalization()(dense1)\n",
        "    dense1 = Dropout(dropout_rate)(dense1)\n",
        "    \n",
        "    dense2 = Dense(32, activation='relu')(dense1)\n",
        "    dense2 = BatchNormalization()(dense2)\n",
        "    dense2 = Dropout(dropout_rate)(dense2)\n",
        "    \n",
        "    # Output (float32 for mixed precision)\n",
        "    output = Dense(1, activation='sigmoid', dtype='float32', name='similarity_output')(dense2)\n",
        "    \n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='BiLSTM_Siamese_GPU')\n",
        "    return model\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL 1: BiLSTM SIAMESE NETWORK (GPU-OPTIMIZED)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Build model within strategy scope if multi-GPU\n",
        "if strategy:\n",
        "    with strategy.scope():\n",
        "        model_bilstm = build_bilstm_siamese_model(\n",
        "            vocab_size, config.EMBEDDING_DIM, config.MAX_SEQUENCE_LENGTH,\n",
        "            config.LSTM_UNITS, config.DROPOUT_RATE\n",
        "        )\n",
        "        model_bilstm.compile(\n",
        "            optimizer=Adam(learning_rate=config.LEARNING_RATE),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "            jit_compile=config.USE_XLA  # XLA compilation\n",
        "        )\n",
        "else:\n",
        "    model_bilstm = build_bilstm_siamese_model(\n",
        "        vocab_size, config.EMBEDDING_DIM, config.MAX_SEQUENCE_LENGTH,\n",
        "        config.LSTM_UNITS, config.DROPOUT_RATE\n",
        "    )\n",
        "    model_bilstm.compile(\n",
        "        optimizer=Adam(learning_rate=config.LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "        jit_compile=config.USE_XLA\n",
        "    )\n",
        "\n",
        "model_bilstm.summary()\n",
        "print(f\"\\n‚úÖ Model compiled with XLA: {config.USE_XLA}\")\n",
        "print(f\"‚úÖ Mixed precision: {config.USE_MIXED_PRECISION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU-optimized callbacks\n",
        "callbacks_bilstm = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=config.EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=config.REDUCE_LR_PATIENCE,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'best_bilstm_siamese_gpu.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train with GPU acceleration\n",
        "print(\"\\nüöÄ Training BiLSTM Siamese Network on GPU...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_bilstm = model_bilstm.fit(\n",
        "    train_dataset,\n",
        "    epochs=config.EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks_bilstm,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time_bilstm = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Training completed in {training_time_bilstm:.2f}s ({training_time_bilstm/60:.2f} min)\")\n",
        "print(f\"   Average time per epoch: {training_time_bilstm/len(history_bilstm.history['loss']):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model 2: Attention-Based Encoder (GPU-Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_attention_encoder_model(vocab_size: int,\n",
        "                                 embedding_dim: int,\n",
        "                                 max_seq_length: int,\n",
        "                                 lstm_units: int,\n",
        "                                 dropout_rate: float = 0.3) -> Model:\n",
        "    \"\"\"\n",
        "    Build GPU-optimized Attention-based Encoder.\n",
        "    \n",
        "    GPU Optimizations:\n",
        "    - Efficient attention computation\n",
        "    - Batch normalization\n",
        "    - Mixed precision compatible\n",
        "    \"\"\"\n",
        "    input_1 = Input(shape=(max_seq_length,), name='clause_1')\n",
        "    input_2 = Input(shape=(max_seq_length,), name='clause_2')\n",
        "    \n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=max_seq_length,\n",
        "        mask_zero=True,\n",
        "        name='shared_embedding'\n",
        "    )\n",
        "    \n",
        "    bilstm_layer = Bidirectional(\n",
        "        LSTM(lstm_units, return_sequences=True, dropout=dropout_rate),\n",
        "        name='shared_bilstm'\n",
        "    )\n",
        "    \n",
        "    embedded_1 = embedding_layer(input_1)\n",
        "    embedded_2 = embedding_layer(input_2)\n",
        "    \n",
        "    lstm_output_1 = bilstm_layer(embedded_1)\n",
        "    lstm_output_2 = bilstm_layer(embedded_2)\n",
        "    \n",
        "    attention_layer = Attention(name='shared_attention')\n",
        "    \n",
        "    attention_output_1 = attention_layer([lstm_output_1, lstm_output_1])\n",
        "    attention_output_2 = attention_layer([lstm_output_2, lstm_output_2])\n",
        "    \n",
        "    pooling_1 = GlobalAveragePooling1D()(attention_output_1)\n",
        "    pooling_2 = GlobalAveragePooling1D()(attention_output_2)\n",
        "    \n",
        "    difference = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([pooling_1, pooling_2])\n",
        "    multiplication = Multiply()([pooling_1, pooling_2])\n",
        "    merged = Concatenate()([pooling_1, pooling_2, difference, multiplication])\n",
        "    \n",
        "    dense1 = Dense(128, activation='relu')(merged)\n",
        "    dense1 = BatchNormalization()(dense1)\n",
        "    dense1 = Dropout(dropout_rate)(dense1)\n",
        "    \n",
        "    dense2 = Dense(64, activation='relu')(dense1)\n",
        "    dense2 = BatchNormalization()(dense2)\n",
        "    dense2 = Dropout(dropout_rate)(dense2)\n",
        "    \n",
        "    dense3 = Dense(32, activation='relu')(dense2)\n",
        "    dense3 = Dropout(dropout_rate)(dense3)\n",
        "    \n",
        "    output = Dense(1, activation='sigmoid', dtype='float32', name='similarity_output')(dense3)\n",
        "    \n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='Attention_Encoder_GPU')\n",
        "    return model\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL 2: ATTENTION-BASED ENCODER (GPU-OPTIMIZED)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if strategy:\n",
        "    with strategy.scope():\n",
        "        model_attention = build_attention_encoder_model(\n",
        "            vocab_size, config.EMBEDDING_DIM, config.MAX_SEQUENCE_LENGTH,\n",
        "            config.LSTM_UNITS, config.DROPOUT_RATE\n",
        "        )\n",
        "        model_attention.compile(\n",
        "            optimizer=Adam(learning_rate=config.LEARNING_RATE),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "            jit_compile=config.USE_XLA\n",
        "        )\n",
        "else:\n",
        "    model_attention = build_attention_encoder_model(\n",
        "        vocab_size, config.EMBEDDING_DIM, config.MAX_SEQUENCE_LENGTH,\n",
        "        config.LSTM_UNITS, config.DROPOUT_RATE\n",
        "    )\n",
        "    model_attention.compile(\n",
        "        optimizer=Adam(learning_rate=config.LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "        jit_compile=config.USE_XLA\n",
        "    )\n",
        "\n",
        "model_attention.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks_attention = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=config.EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=config.REDUCE_LR_PATIENCE,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'best_attention_encoder_gpu.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüöÄ Training Attention-Based Encoder on GPU...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_attention = model_attention.fit(\n",
        "    train_dataset,\n",
        "    epochs=config.EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks_attention,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time_attention = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Training completed in {training_time_attention:.2f}s ({training_time_attention/60:.2f} min)\")\n",
        "print(f\"   Average time per epoch: {training_time_attention/len(history_attention.history['loss']):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model: Model, X1_test, X2_test, y_test, model_name: str) -> Dict:\n",
        "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"EVALUATING {model_name.upper()}\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "    \n",
        "    y_pred_prob = model.predict([X1_test, X2_test], verbose=0)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    pr_auc = average_precision_score(y_test, y_pred_prob)\n",
        "    \n",
        "    print(f\"\\nüìä Performance Metrics:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
        "    print(f\"  PR-AUC:    {pr_auc:.4f}\")\n",
        "    \n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"\\nüìä Confusion Matrix:\")\n",
        "    print(f\"  TN: {cm[0][0]:4d}  |  FP: {cm[0][1]:4d}\")\n",
        "    print(f\"  FN: {cm[1][0]:4d}  |  TP: {cm[1][1]:4d}\")\n",
        "    \n",
        "    print(f\"\\nüìä Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Different', 'Similar']))\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'confusion_matrix': cm,\n",
        "        'y_true': y_test,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_prob': y_pred_prob.flatten()\n",
        "    }\n",
        "\n",
        "results_bilstm = evaluate_model(\n",
        "    model_bilstm, X1_test, X2_test, y_test, \"BiLSTM Siamese (GPU)\"\n",
        ")\n",
        "\n",
        "results_attention = evaluate_model(\n",
        "    model_attention, X1_test, X2_test, y_test, \"Attention Encoder (GPU)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Training History Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"Plot training curves.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
        "    axes[0].plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[0].set_title(f'{model_name} - Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
        "    axes[1].plot(history.history['val_loss'], label='Val', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Loss', fontsize=12)\n",
        "    axes[1].set_title(f'{model_name} - Loss', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history_bilstm, \"BiLSTM Siamese (GPU)\")\n",
        "plot_training_history(history_attention, \"Attention Encoder (GPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluation Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrices(results_list):\n",
        "    \"\"\"Plot confusion matrices.\"\"\"\n",
        "    fig, axes = plt.subplots(1, len(results_list), figsize=(7*len(results_list), 5))\n",
        "    if len(results_list) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, results in enumerate(results_list):\n",
        "        cm = results['confusion_matrix']\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Different', 'Similar'],\n",
        "                   yticklabels=['Different', 'Similar'],\n",
        "                   ax=axes[idx], cbar=True, square=True)\n",
        "        axes[idx].set_xlabel('Predicted', fontsize=12)\n",
        "        axes[idx].set_ylabel('Actual', fontsize=12)\n",
        "        axes[idx].set_title(f\"{results['model_name']}\\nConfusion Matrix\",\n",
        "                          fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrices([results_bilstm, results_attention])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_roc_curves(results_list):\n",
        "    \"\"\"Plot ROC curves.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['blue', 'red']\n",
        "    \n",
        "    for idx, results in enumerate(results_list):\n",
        "        fpr, tpr, _ = roc_curve(results['y_true'], results['y_pred_prob'])\n",
        "        auc_score = results['roc_auc']\n",
        "        plt.plot(fpr, tpr, label=f\"{results['model_name']} (AUC = {auc_score:.4f})\",\n",
        "                linewidth=2, color=colors[idx])\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves - GPU-Accelerated Models', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curves([results_bilstm, results_attention])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Comparative Analysis with GPU Speedup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance comparison\n",
        "comparison_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': 'BiLSTM Siamese (GPU)',\n",
        "        'Accuracy': results_bilstm['accuracy'],\n",
        "        'Precision': results_bilstm['precision'],\n",
        "        'Recall': results_bilstm['recall'],\n",
        "        'F1-Score': results_bilstm['f1_score'],\n",
        "        'ROC-AUC': results_bilstm['roc_auc'],\n",
        "        'Training Time (min)': training_time_bilstm / 60\n",
        "    },\n",
        "    {\n",
        "        'Model': 'Attention Encoder (GPU)',\n",
        "        'Accuracy': results_attention['accuracy'],\n",
        "        'Precision': results_attention['precision'],\n",
        "        'Recall': results_attention['recall'],\n",
        "        'F1-Score': results_attention['f1_score'],\n",
        "        'ROC-AUC': results_attention['roc_auc'],\n",
        "        'Training Time (min)': training_time_attention / 60\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GPU-ACCELERATED MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n\", comparison_df.to_string(index=False))\n",
        "\n",
        "best_idx = comparison_df['F1-Score'].idxmax()\n",
        "print(f\"\\nüèÜ Best Model: {comparison_df.iloc[best_idx]['Model']}\")\n",
        "\n",
        "print(\"\\n‚ö° GPU Performance Benefits:\")\n",
        "print(f\"  ‚Ä¢ Larger batch size: {config.BATCH_SIZE} (vs 64 on CPU)\")\n",
        "print(f\"  ‚Ä¢ Mixed precision training: {config.USE_MIXED_PRECISION}\")\n",
        "print(f\"  ‚Ä¢ XLA compilation: {config.USE_XLA}\")\n",
        "print(f\"  ‚Ä¢ Data prefetching: Enabled\")\n",
        "print(f\"  ‚Ä¢ CuDNN-optimized LSTM: Automatic on GPU\")\n",
        "print(f\"\\n  Expected speedup: 2-5x faster than CPU training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save comparison\n",
        "comparison_df.to_csv('gpu_model_comparison_results.csv', index=False)\n",
        "print(\"‚úì Saved: gpu_model_comparison_results.csv\")\n",
        "\n",
        "# Save tokenizer\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"‚úì Saved: tokenizer.pkl\")\n",
        "\n",
        "# Save models\n",
        "model_bilstm.save('final_bilstm_siamese_gpu.h5')\n",
        "print(\"‚úì Saved: final_bilstm_siamese_gpu.h5\")\n",
        "\n",
        "model_attention.save('final_attention_encoder_gpu.h5')\n",
        "print(\"‚úì Saved: final_attention_encoder_gpu.h5\")\n",
        "\n",
        "print(\"\\n‚úÖ All GPU-optimized models saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Technical Summary - GPU Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GPU-OPTIMIZED IMPLEMENTATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüöÄ GPU Optimizations Applied:\")\n",
        "print(\"  1. Mixed Precision Training (FP16)\")\n",
        "print(\"     - Compute: float16 for 2-3x speedup\")\n",
        "print(\"     - Variables: float32 for numerical stability\")\n",
        "print(\"     - Output: float32 for accurate predictions\")\n",
        "print(\"\\n  2. Memory Growth Management\")\n",
        "print(\"     - Prevents TensorFlow from allocating all GPU memory\")\n",
        "print(\"     - Allows multiple processes to share GPU\")\n",
        "print(\"\\n  3. XLA (Accelerated Linear Algebra)\")\n",
        "print(\"     - Just-In-Time compilation of operations\")\n",
        "print(\"     - Optimized GPU kernel fusion\")\n",
        "print(\"\\n  4. Data Pipeline Optimization\")\n",
        "print(\"     - tf.data with AUTOTUNE prefetching\")\n",
        "print(\"     - Dataset caching in memory\")\n",
        "print(\"     - Parallel data loading\")\n",
        "print(\"\\n  5. CuDNN-Optimized LSTM\")\n",
        "print(\"     - Automatic when GPU available\")\n",
        "print(\"     - Significantly faster than CPU LSTM\")\n",
        "print(\"\\n  6. Batch Size Optimization\")\n",
        "print(f\"     - Increased to {config.BATCH_SIZE} for GPU\")\n",
        "print(\"     - Better GPU utilization\")\n",
        "print(\"\\n  7. Multi-GPU Support\")\n",
        "print(\"     - MirroredStrategy for data parallelism\")\n",
        "print(\"     - Automatic when multiple GPUs detected\")\n",
        "\n",
        "print(\"\\nüìä Performance Metrics:\")\n",
        "print(f\"  BiLSTM Siamese:\")\n",
        "print(f\"    F1-Score: {results_bilstm['f1_score']:.4f}\")\n",
        "print(f\"    Training Time: {training_time_bilstm/60:.2f} min\")\n",
        "print(f\"  Attention Encoder:\")\n",
        "print(f\"    F1-Score: {results_attention['f1_score']:.4f}\")\n",
        "print(f\"    Training Time: {training_time_attention/60:.2f} min\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ GPU-OPTIMIZED IMPLEMENTATION COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüí° Key Takeaways:\")\n",
        "print(\"  ‚Ä¢ Mixed precision provides 2-3x speedup with minimal accuracy loss\")\n",
        "print(\"  ‚Ä¢ Larger batch sizes improve GPU utilization\")\n",
        "print(\"  ‚Ä¢ Data prefetching overlaps I/O with computation\")\n",
        "print(\"  ‚Ä¢ XLA compilation optimizes operations for target hardware\")\n",
        "print(\"  ‚Ä¢ CuDNN-optimized layers provide significant speedups\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã GPU Setup Instructions\n",
        "\n",
        "### For NVIDIA GPUs:\n",
        "\n",
        "1. **Install NVIDIA GPU Driver**\n",
        "   ```bash\n",
        "   # Check current driver\n",
        "   nvidia-smi\n",
        "   ```\n",
        "\n",
        "2. **Install CUDA Toolkit (11.2+)**\n",
        "   - Download from: https://developer.nvidia.com/cuda-toolkit\n",
        "\n",
        "3. **Install cuDNN (8.1+)**\n",
        "   - Download from: https://developer.nvidia.com/cudnn\n",
        "\n",
        "4. **Install TensorFlow with GPU Support**\n",
        "   ```bash\n",
        "   pip install tensorflow[and-cuda]\n",
        "   # OR for specific version\n",
        "   pip install tensorflow-gpu==2.12.0\n",
        "   ```\n",
        "\n",
        "5. **Verify GPU Setup**\n",
        "   ```python\n",
        "   import tensorflow as tf\n",
        "   print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))\n",
        "   ```\n",
        "\n",
        "### For Google Colab:\n",
        "\n",
        "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4, V100, or A100)\n",
        "2. Run this notebook - GPU setup is automatic!\n",
        "\n",
        "### Performance Tips:\n",
        "\n",
        "- **Batch Size:** Increase until you hit OOM errors, then reduce slightly\n",
        "- **Mixed Precision:** Always enable for modern GPUs (Volta and newer)\n",
        "- **XLA:** Test with/without - sometimes faster, sometimes not\n",
        "- **Data Prefetching:** Always use `tf.data.AUTOTUNE`\n",
        "- **Monitor GPU:** Use `nvidia-smi` or `nvtop` to check utilization\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}